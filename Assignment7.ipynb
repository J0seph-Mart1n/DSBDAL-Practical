{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assignment 7\n",
    "\n",
    "Title of the Assignment:\n",
    "1. Extract Sample document and apply following document preprocessing methods:\n",
    "Tokenization, POS Tagging, stop words removal, Stemming and Lemmatization.\n",
    "2. Create representation of document by calculating Term Frequency and Inverse Document\n",
    "Frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Downloading Required packages</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\marti\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\marti\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\marti\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\marti\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text= \"Tokenization is the first step in text analytics. The process of breaking down a text paragraph into smaller chunks such as words or sentences is called Tokenization.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Performing Tokenization</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tokenization is the first step in text analytics.', 'The process of breaking down a text paragraph into smaller chunks such as words or sentences is called Tokenization.']\n",
      "['Tokenization', 'is', 'the', 'first', 'step', 'in', 'text', 'analytics', '.', 'The', 'process', 'of', 'breaking', 'down', 'a', 'text', 'paragraph', 'into', 'smaller', 'chunks', 'such', 'as', 'words', 'or', 'sentences', 'is', 'called', 'Tokenization', '.']\n"
     ]
    }
   ],
   "source": [
    "#Sentence Tokenization\n",
    "from nltk.tokenize import sent_tokenize\n",
    "tokenized_text= sent_tokenize(text)\n",
    "print(tokenized_text)\n",
    "#Word Tokenization\n",
    "from nltk.tokenize import word_tokenize\n",
    "tokenized_word=word_tokenize(text)\n",
    "print(tokenized_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Removing Punctuations and stop words</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'theirs', 'through', 'down', 'shouldn', 'you', 'or', 'some', 'with', 'again', 'about', 'isn', 'whom', \"don't\", 'wouldn', 'myself', 's', 'there', 'herself', 'him', 'same', 'themselves', 'be', 'haven', 'if', 'am', 'here', 'your', 'those', 'were', 'ain', 'as', 'where', 'while', 'mustn', 'no', 'yours', \"couldn't\", 'so', \"haven't\", 'shan', 'and', 'this', 'at', 'just', 'aren', 'them', 'won', 'our', 'in', 'once', 'ourselves', 'should', 'against', 'who', 'that', \"isn't\", 'is', 'over', 'own', 'further', 'below', \"she's\", \"you're\", 'having', 'm', \"that'll\", 'any', \"doesn't\", 'had', 'these', 'into', 'now', 'such', \"needn't\", 'was', 'we', 'between', 'she', 'they', 'hers', 'of', 're', \"shan't\", \"you've\", 'the', 'until', 'a', 'what', 'than', 'are', 'weren', 'her', 't', 'not', 'its', 'off', 'it', 'an', \"it's\", \"aren't\", 'himself', 'only', 'few', 'nor', \"you'd\", 'then', \"won't\", 'under', 'needn', 'for', \"weren't\", 'y', 'yourselves', \"hasn't\", 'couldn', \"you'll\", \"hadn't\", \"shouldn't\", 'me', 'hasn', 'both', 'll', 'my', 'ma', 'will', 'mightn', 'when', 'up', 'on', 'all', 'hadn', 'he', 'how', 'but', 'didn', \"didn't\", 'their', \"mightn't\", 'i', \"wasn't\", 'o', 'doesn', 'ours', 'have', 'do', 'because', 'can', 'his', 'does', 'from', 'to', 'yourself', 'each', 'by', 'being', 'out', 'which', 'why', 've', 'most', 'very', 'don', \"wouldn't\", 'doing', 'been', 'itself', 'before', 'other', \"should've\", 'wasn', 'has', 'during', 'd', \"mustn't\", 'too', 'after', 'did', 'more', 'above'}\n",
      "Tokenized Sentence: ['tokenization', 'is', 'the', 'first', 'step', 'in', 'text', 'analytics', 'the', 'process', 'of', 'breaking', 'down', 'a', 'text', 'paragraph', 'into', 'smaller', 'chunks', 'such', 'as', 'words', 'or', 'sentences', 'is', 'called', 'tokenization']\n",
      "Filterd Sentence: ['tokenization', 'first', 'step', 'text', 'analytics', 'process', 'breaking', 'text', 'paragraph', 'smaller', 'chunks', 'words', 'sentences', 'called', 'tokenization']\n"
     ]
    }
   ],
   "source": [
    "# print stop words of English\n",
    "from nltk.corpus import stopwords\n",
    "stop_words=set(stopwords.words(\"english\"))\n",
    "print(stop_words)\n",
    "text= \"Tokenization is the first step in text analytics. The process of breaking down a text paragraph into smaller chunks such as words or sentences is called Tokenization.\"\n",
    "text= re.sub('[^a-zA-Z]', ' ',text)\n",
    "tokens = word_tokenize(text.lower())\n",
    "filtered_text=[]\n",
    "for w in tokens:\n",
    "    if w not in stop_words:\n",
    "        filtered_text.append(w)\n",
    "print(\"Tokenized Sentence:\",tokens)\n",
    "print(\"Filterd Sentence:\",filtered_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Performing Stemming</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['token', 'first', 'step', 'text', 'analyt', 'process', 'break', 'text', 'paragraph', 'smaller', 'chunk', 'word', 'sentenc', 'call', 'token']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "ps =PorterStemmer()\n",
    "rootWord = []\n",
    "for w in filtered_text:\n",
    "    rootWord.append(ps.stem(w))\n",
    "print(rootWord)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Performing Lemmatization</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\marti\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemma for Tokenization is Tokenization\n",
      "Lemma for is is is\n",
      "Lemma for the is the\n",
      "Lemma for first is first\n",
      "Lemma for step is step\n",
      "Lemma for in is in\n",
      "Lemma for text is text\n",
      "Lemma for analytics is analytics\n",
      "Lemma for . is .\n",
      "Lemma for The is The\n",
      "Lemma for process is process\n",
      "Lemma for of is of\n",
      "Lemma for breaking is breaking\n",
      "Lemma for down is down\n",
      "Lemma for a is a\n",
      "Lemma for text is text\n",
      "Lemma for paragraph is paragraph\n",
      "Lemma for into is into\n",
      "Lemma for smaller is smaller\n",
      "Lemma for chunks is chunk\n",
      "Lemma for such is such\n",
      "Lemma for as is a\n",
      "Lemma for words is word\n",
      "Lemma for or is or\n",
      "Lemma for sentences is sentence\n",
      "Lemma for is is is\n",
      "Lemma for called is called\n",
      "Lemma for Tokenization is Tokenization\n",
      "Lemma for . is .\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "text= \"Tokenization is the first step in text analytics. The process of breaking down a text paragraph into smaller chunks such as words or sentences is called Tokenization.\"\n",
    "tokenization = nltk.word_tokenize(text)\n",
    "for w in tokenization:\n",
    "    print(\"Lemma for {} is {}\".format(w, wordnet_lemmatizer.lemmatize(w)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Apply POS Tagging to text</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Tokenization', 'NN')]\n",
      "[('is', 'VBZ')]\n",
      "[('the', 'DT')]\n",
      "[('first', 'RB')]\n",
      "[('step', 'NN')]\n",
      "[('in', 'IN')]\n",
      "[('text', 'NN')]\n",
      "[('analytics', 'NNS')]\n",
      "[('.', '.')]\n",
      "[('The', 'DT')]\n",
      "[('process', 'NN')]\n",
      "[('of', 'IN')]\n",
      "[('breaking', 'VBG')]\n",
      "[('down', 'RB')]\n",
      "[('a', 'DT')]\n",
      "[('text', 'NN')]\n",
      "[('paragraph', 'NN')]\n",
      "[('into', 'IN')]\n",
      "[('smaller', 'JJR')]\n",
      "[('chunks', 'NNS')]\n",
      "[('such', 'JJ')]\n",
      "[('as', 'IN')]\n",
      "[('words', 'NNS')]\n",
      "[('or', 'CC')]\n",
      "[('sentences', 'NNS')]\n",
      "[('is', 'VBZ')]\n",
      "[('called', 'VBN')]\n",
      "[('Tokenization', 'NN')]\n",
      "[('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "text= \"Tokenization is the first step in text analytics. The process of breaking down a text paragraph into smaller chunks such as words or sentences is called Tokenization.\"\n",
    "words=word_tokenize(text)\n",
    "for word in words:\n",
    "    print(nltk.pos_tag([word]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Calculating TF/IDF</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "documentA = 'Jupiter is the largest Planet'\n",
    "documentB = 'Mars is the fourth planet from the Sun'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "bagOfWordsA = documentA.split(' ')\n",
    "bagOfWordsB = documentB.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Jupiter', 'is', 'the', 'largest', 'Planet']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bagOfWordsA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Mars', 'is', 'the', 'fourth', 'planet', 'from', 'the', 'Sun']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bagOfWordsB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniqueWords = set(bagOfWordsA).union(set(bagOfWordsB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Jupiter',\n",
       " 'Mars',\n",
       " 'Planet',\n",
       " 'Sun',\n",
       " 'fourth',\n",
       " 'from',\n",
       " 'is',\n",
       " 'largest',\n",
       " 'planet',\n",
       " 'the'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uniqueWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "numOfWordsA = dict.fromkeys(uniqueWords, 0)\n",
    "for word in bagOfWordsA:\n",
    "    numOfWordsA[word] += 1\n",
    "\n",
    "numOfWordsB = dict.fromkeys(uniqueWords, 0)    \n",
    "for word in bagOfWordsB:\n",
    "    numOfWordsB[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Mars': 0,\n",
       " 'Jupiter': 1,\n",
       " 'is': 1,\n",
       " 'Sun': 0,\n",
       " 'planet': 0,\n",
       " 'from': 0,\n",
       " 'Planet': 1,\n",
       " 'largest': 1,\n",
       " 'fourth': 0,\n",
       " 'the': 1}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numOfWordsA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Mars': 1,\n",
       " 'Jupiter': 0,\n",
       " 'is': 1,\n",
       " 'Sun': 1,\n",
       " 'planet': 1,\n",
       " 'from': 1,\n",
       " 'Planet': 0,\n",
       " 'largest': 0,\n",
       " 'fourth': 1,\n",
       " 'the': 2}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numOfWordsB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Compute the term frequency</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeTF(wordDict, bagOfWords):\n",
    "    tfDict = {}\n",
    "    bagOfWordsCount = len(bagOfWords)\n",
    "    for word, count in wordDict.items():\n",
    "        tfDict[word] = count / float(bagOfWordsCount)\n",
    "    return tfDict\n",
    "tfA = computeTF(numOfWordsA, bagOfWordsA)\n",
    "tfB = computeTF(numOfWordsB, bagOfWordsB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Mars': 0.0, 'Jupiter': 0.2, 'is': 0.2, 'Sun': 0.0, 'planet': 0.0, 'from': 0.0, 'Planet': 0.2, 'largest': 0.2, 'fourth': 0.0, 'the': 0.2}\n"
     ]
    }
   ],
   "source": [
    "print(tfA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Mars': 0.125, 'Jupiter': 0.0, 'is': 0.125, 'Sun': 0.125, 'planet': 0.125, 'from': 0.125, 'Planet': 0.0, 'largest': 0.0, 'fourth': 0.125, 'the': 0.25}\n"
     ]
    }
   ],
   "source": [
    "print(tfB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Compute the term inverse document frequency</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Mars': 0.6931471805599453,\n",
       " 'Jupiter': 0.6931471805599453,\n",
       " 'is': 0.0,\n",
       " 'Sun': 0.6931471805599453,\n",
       " 'planet': 0.6931471805599453,\n",
       " 'from': 0.6931471805599453,\n",
       " 'Planet': 0.6931471805599453,\n",
       " 'largest': 0.6931471805599453,\n",
       " 'fourth': 0.6931471805599453,\n",
       " 'the': 0.0}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def computeIDF(documents):\n",
    "    N = len(documents)\n",
    "    idfDict = dict.fromkeys(documents[0].keys(), 0)\n",
    "    for document in documents:\n",
    "        for word, val in document.items():\n",
    "            if val > 0:\n",
    "                idfDict[word] += 1\n",
    "    for word, val in idfDict.items():\n",
    "        idfDict[word] = math.log(N / float(val))\n",
    "    return idfDict\n",
    "\n",
    "idfs = computeIDF([numOfWordsA, numOfWordsB])\n",
    "idfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Compute the term TF/IDF for all words</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mars</th>\n",
       "      <th>Jupiter</th>\n",
       "      <th>is</th>\n",
       "      <th>Sun</th>\n",
       "      <th>planet</th>\n",
       "      <th>from</th>\n",
       "      <th>Planet</th>\n",
       "      <th>largest</th>\n",
       "      <th>fourth</th>\n",
       "      <th>the</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.138629</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.138629</td>\n",
       "      <td>0.138629</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.086643</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.086643</td>\n",
       "      <td>0.086643</td>\n",
       "      <td>0.086643</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.086643</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Mars   Jupiter   is       Sun    planet      from    Planet   largest  \\\n",
       "0  0.000000  0.138629  0.0  0.000000  0.000000  0.000000  0.138629  0.138629   \n",
       "1  0.086643  0.000000  0.0  0.086643  0.086643  0.086643  0.000000  0.000000   \n",
       "\n",
       "     fourth  the  \n",
       "0  0.000000  0.0  \n",
       "1  0.086643  0.0  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def computeTFIDF(tfBagOfWords, idfs):\n",
    "    tfidf = {}\n",
    "    for word, val in tfBagOfWords.items():\n",
    "        tfidf[word] = val * idfs[word]\n",
    "    return tfidf\n",
    "\n",
    "tfidfA = computeTFIDF(tfA, idfs)\n",
    "tfidfB = computeTFIDF(tfB, idfs)\n",
    "df = pd.DataFrame([tfidfA, tfidfB])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
